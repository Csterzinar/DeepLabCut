{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RK255E7YoEIt"
   },
   "source": [
    "# DeepLabCut Toolbox\n",
    "https://github.com/DeepLabCut/DeepLabCut\n",
    "\n",
    "This notebook demonstrates the necessary steps to use DeepLabCut modified for the WMAZE\n",
    "\n",
    "If you already have a trained network jump to the analyze videos section or the bottom - Batch Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Uoz9mdPoEIy"
   },
   "source": [
    "## Create a new project\n",
    "This function creates a new project with subdirectories and a basic configuration file in the user defined directory otherwise the project is created in the current working directory.\n",
    "You can always add new videos (for labeling more data) to the project at any stage of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jqLZhp7EoEI0",
    "ExecuteTime": {
     "start_time": "2024-11-25T11:04:55.553928Z",
     "end_time": "2024-11-25T11:05:12.382179Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import deeplabcut\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c9DjG55FoEI7",
    "ExecuteTime": {
     "start_time": "2024-11-25T11:08:54.322552Z",
     "end_time": "2024-11-25T11:08:54.973449Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created \"C:\\Users\\sahanasrivathsa\\Documents\\SYNC\\Work\\BarnesLab\\CODE\\DEEPLABCUT\\WMaze-SS-2024-11-25\\videos\"\n",
      "Created \"C:\\Users\\sahanasrivathsa\\Documents\\SYNC\\Work\\BarnesLab\\CODE\\DEEPLABCUT\\WMaze-SS-2024-11-25\\labeled-data\"\n",
      "Created \"C:\\Users\\sahanasrivathsa\\Documents\\SYNC\\Work\\BarnesLab\\CODE\\DEEPLABCUT\\WMaze-SS-2024-11-25\\training-datasets\"\n",
      "Created \"C:\\Users\\sahanasrivathsa\\Documents\\SYNC\\Work\\BarnesLab\\CODE\\DEEPLABCUT\\WMaze-SS-2024-11-25\\dlc-models\"\n",
      "1  videos from the directory C:\\Users\\sahanasrivathsa\\Documents\\SYNC\\Work\\BarnesLab\\CODE\\DEEPLABCUT\\CutVideosForTraining were added to the project.\n",
      "Copying the videos\n",
      "C:\\Users\\sahanasrivathsa\\Documents\\SYNC\\Work\\BarnesLab\\CODE\\DEEPLABCUT\\WMaze-SS-2024-11-25\\videos\\VT1.mp4\n",
      "Generated \"C:\\Users\\sahanasrivathsa\\Documents\\SYNC\\Work\\BarnesLab\\CODE\\DEEPLABCUT\\WMaze-SS-2024-11-25\\config.yaml\"\n",
      "\n",
      "A new project with name WMaze-SS-2024-11-25 is created at C:\\Users\\sahanasrivathsa\\Documents\\SYNC\\Work\\BarnesLab\\CODE\\DEEPLABCUT and a configurable file (config.yaml) is stored there. Change the parameters in this file to adapt to your project's needs.\n",
      " Once you have changed the configuration file, use the function 'extract_frames' to select frames for labeling.\n",
      ". [OPTIONAL] Use the function 'add_new_videos' to add new videos to your project (at any stage).\n"
     ]
    }
   ],
   "source": [
    "task='WMaze' # Enter the name of your experiment Task\n",
    "experimenter='SS' # Enter the name of the experimenter\n",
    "video=[r'C:\\Users\\sahanasrivathsa\\Documents\\SYNC\\Work\\BarnesLab\\CODE\\DEEPLABCUT\\CutVideosForTraining'] # Enter the paths of your videos OR FOLDER you want to grab frames from. ONLY FRAMES FOR INITIAL NOT ALL VIDEOS\n",
    "working_dir=r'C:\\Users\\sahanasrivathsa\\Documents\\SYNC\\Work\\BarnesLab\\CODE\\DEEPLABCUT' # Directory where the file should be created\n",
    "path_config_file=deeplabcut.create_new_project(task,experimenter,video,working_dir,copy_videos=True)\n",
    "\n",
    "# NOTE: The function returns the path, where your project is. \n",
    "# You could also enter this manually (e.g. if the project is already created and you want to pick up, where you stopped...)\n",
    "#path_config_file = '/home/Mackenzie/Reaching/config.yaml' # Enter the path of the config file that was just created from the above step (check the folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, go edit the config.yaml file that was created! \n",
    "Add your body part labels, edit the number of frames to extract per video, etc.\n",
    "\n",
    "### Note that you can see more information about ANY function by adding a ? at the end,  i.e.\n",
    "- e.g. deeplabcut.extract_frames?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0yXW0bx1oEJA"
   },
   "source": [
    "## Extract frames from videos \n",
    "A key point for a successful feature detector is to select diverse frames, which are typical for the behavior you study that should be labeled.\n",
    "This function selects N frames either uniformly sampled from a particular video (or folder) ('uniform'). Note: this might not yield diverse frames, if the behavior is sparsely distributed (consider using kmeans), and/or select frames manually etc.\n",
    "Also make sure to get select data from different (behavioral) sessions and different animals if those vary substantially (to train an invariant feature detector).\n",
    "Individual images should not be too big (i.e. < 850 x 850 pixel). Although this can be taken care of later as well, it is advisable to crop the frames, to remove unnecessary parts of the frame as much as possible.\n",
    "Always check the output of cropping. If you are happy with the results proceed to labeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t1ulumCuoEJC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config file read successfully.\n",
      "Do you want to extract (perhaps additional) frames for video: C:\\Users\\sahanasrivathsa\\Documents\\SYNC\\Work\\BarnesLab\\CODE\\DEEPLABCUT\\WMaze-SS-2024-11-25\\videos\\VT1.mp4 ?\n",
      "Extracting frames based on kmeans ...\n",
      "Kmeans-quantization based extracting of frames from 0.0  seconds to 4745.13  seconds.\n",
      "Extracting and downsampling... 142354  frames from the video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2560it [00:23, 108.29it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 5\u001B[0m\n\u001B[0;32m      1\u001B[0m get_ipython()\u001B[38;5;241m.\u001B[39mrun_line_magic(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmatplotlib\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124minline\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m#there are other ways to grab frames, such as uniformly; please see the paper:\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m#AUTOMATIC:\u001B[39;00m\n\u001B[1;32m----> 5\u001B[0m \u001B[43mdeeplabcut\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mextract_frames\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath_config_file\u001B[49m\u001B[43m,\u001B[49m\u001B[43malgo\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mkmeans\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43mslider_width\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m15\u001B[39;49m\u001B[43m)\u001B[49m \n",
      "File \u001B[1;32m~\\Documents\\SYNC\\Work\\BarnesLab\\CODE\\Github\\DeepLabCut\\deeplabcut\\generate_training_dataset\\frame_extraction.py:383\u001B[0m, in \u001B[0;36mextract_frames\u001B[1;34m(config, mode, algo, crop, userfeedback, cluster_step, cluster_resizewidth, cluster_color, opencv, slider_width, config3d, extracted_cam, videos_list)\u001B[0m\n\u001B[0;32m    381\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m algo \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mkmeans\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    382\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m opencv:\n\u001B[1;32m--> 383\u001B[0m         frames2pick \u001B[38;5;241m=\u001B[39m \u001B[43mframeselectiontools\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mKmeansbasedFrameselectioncv2\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    384\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcap\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    385\u001B[0m \u001B[43m            \u001B[49m\u001B[43mnumframes2pick\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    386\u001B[0m \u001B[43m            \u001B[49m\u001B[43mstart\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    387\u001B[0m \u001B[43m            \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    388\u001B[0m \u001B[43m            \u001B[49m\u001B[43mstep\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcluster_step\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    389\u001B[0m \u001B[43m            \u001B[49m\u001B[43mresizewidth\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcluster_resizewidth\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    390\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcolor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcluster_color\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    391\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    392\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    393\u001B[0m         frames2pick \u001B[38;5;241m=\u001B[39m frameselectiontools\u001B[38;5;241m.\u001B[39mKmeansbasedFrameselection(\n\u001B[0;32m    394\u001B[0m             clip,\n\u001B[0;32m    395\u001B[0m             numframes2pick,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    400\u001B[0m             color\u001B[38;5;241m=\u001B[39mcluster_color,\n\u001B[0;32m    401\u001B[0m         )\n",
      "File \u001B[1;32m~\\Documents\\SYNC\\Work\\BarnesLab\\CODE\\Github\\DeepLabCut\\deeplabcut\\utils\\frameselectiontools.py:347\u001B[0m, in \u001B[0;36mKmeansbasedFrameselectioncv2\u001B[1;34m(cap, numframes2pick, start, stop, Index, step, resizewidth, batchsize, max_iter, color)\u001B[0m\n\u001B[0;32m    344\u001B[0m frame \u001B[38;5;241m=\u001B[39m cap\u001B[38;5;241m.\u001B[39mread_frame(crop\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m    345\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m frame \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    346\u001B[0m     image \u001B[38;5;241m=\u001B[39m img_as_ubyte(\n\u001B[1;32m--> 347\u001B[0m         \u001B[43mcv2\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresize\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    348\u001B[0m \u001B[43m            \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    349\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    350\u001B[0m \u001B[43m            \u001B[49m\u001B[43mfx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mratio\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    351\u001B[0m \u001B[43m            \u001B[49m\u001B[43mfy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mratio\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    352\u001B[0m \u001B[43m            \u001B[49m\u001B[43minterpolation\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcv2\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mINTER_NEAREST\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    353\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    354\u001B[0m     )  \u001B[38;5;66;03m# color trafo not necessary; lack thereof improves speed.\u001B[39;00m\n\u001B[0;32m    355\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m    356\u001B[0m         \u001B[38;5;129;01mnot\u001B[39;00m allocated\n\u001B[0;32m    357\u001B[0m     ):  \u001B[38;5;66;03m#'DATA' not in locals(): #allocate memory in first pass\u001B[39;00m\n\u001B[0;32m    358\u001B[0m         DATA \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mempty(\n\u001B[0;32m    359\u001B[0m             (nframes, np\u001B[38;5;241m.\u001B[39mshape(image)[\u001B[38;5;241m0\u001B[39m], np\u001B[38;5;241m.\u001B[39mshape(image)[\u001B[38;5;241m1\u001B[39m])\n\u001B[0;32m    360\u001B[0m         )\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "#there are other ways to grab frames, such as uniformly; please see the paper:\n",
    "\n",
    "#AUTOMATIC:\n",
    "deeplabcut.extract_frames(path_config_file,algo=\"kmeans\",slider_width=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gjn6ZDonoEJH"
   },
   "source": [
    "## Label the extracted frames\n",
    "\n",
    "Only videos in the config file can be used to extract the frames. Extracted labels for each video are stored in the project directory under the subdirectory **'labeled-data'**. Each subdirectory is named after the name of the video. The toolbox has a labeling toolbox which could be used for labeling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iyROSOiEoEJI"
   },
   "outputs": [],
   "source": [
    "# napari will pop up!\n",
    "# Please go to plugin > deeplabcut to start\n",
    "# then, drag-and-drop the project configuration file into the viewer (the value of path_config_file)\n",
    "# finally, drop the folder containing the images (in 'labeled-data') in the viewer\n",
    "\n",
    "%gui qt6\n",
    "import napari\n",
    "napari.Viewer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vim95ZvkPSeN"
   },
   "source": [
    "## Check the labels\n",
    "[OPTIONAL] Checking if the labels were created and stored correctly is beneficial for training, since labeling is one of the most critical parts for creating the training dataset. The DeepLabCut toolbox provides a function `check\\_labels'  to do so. It is used as follows:\n",
    "If the labels need adjusted, you can use relauch the labeling GUI to move them around, save, and re-plot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NwvgPJouPP2O"
   },
   "outputs": [],
   "source": [
    "deeplabcut.check_labels(path_config_file) #this creates a subdirectory with the frames + your labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Additional Check where the distance between two body parts is greater than a given threshold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "max_dist = 100\n",
    "df = pd.read_hdf('path_to_your_labeled_data_file')\n",
    "bpt1 = df.xs('neck', level='bodyparts', axis=1).to_numpy()\n",
    "bpt2 = df.xs('tail', level='bodyparts', axis=1).to_numpy()\n",
    "# We calculate the vectors from a point to the other\n",
    "# and group them per frame and per animal.\n",
    "try:\n",
    "    diff = (bpt1 - bpt2).reshape((len(df), -1, 2))\n",
    "except ValueError:\n",
    "    diff = (bpt1 - bpt2).reshape((len(df), -1, 3))\n",
    "dist = np.linalg.norm(diff, axis=2)\n",
    "mask = np.any(dist >= max_dist, axis=1)\n",
    "flagged_frames = df.iloc[mask].index"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xNi9s1dboEJN"
   },
   "source": [
    "## Image Augmentation\n",
    "Image augmentation is the process of artificially expanding the training set by applying various transformations to images (e.g., rotation or rescaling) in order to make models more robust and more accurate. The following section of code is not necessary. Only pass if you want more augmentation - It edits the pose_cfg.yaml Some other parameters that might be useful to change are as follows\n",
    "- ``sharpening`` - default set to False can set to true\n",
    "- ``sharpenratio`` - default set to 0.3\n",
    "- `` edge `` - default set to false - edge contrast of image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Image Augmentation Code -  Add more image augmentation if needed\n",
    "import deeplabcut\n",
    "train_pose_config, _ = deeplabcut.return_train_network_path(path_config_file)\n",
    "augs = {\n",
    "    \"gaussian_noise\": True,\n",
    "    \"elastic_transform\": True,\n",
    "    \"rotation\": 180,\n",
    "    \"covering\": True,\n",
    "    \"motion_blur\": True,\n",
    "}\n",
    "deeplabcut.auxiliaryfunctions.edit_config(\n",
    "    train_pose_config,\n",
    "    augs,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Creation of training data set\n",
    "This function generates the training data information for network training based on the pandas dataframes that hold label information. The user can set the fraction of the training set size (from all labeled image in the hd5 file) in the config.yaml file.After running this script the training dataset is created and saved in the project directory under the subdirectory **'training-datasets'**\n",
    "- Multiple shuffles to run different iterations -benchmark performance. Default is 1 -parameterers to change in the pose_cfg.yaml in the train folder are below\n",
    "- `` freeze_bn_stats`` - default is true which is good for a CPU but make false for a GPU\n",
    "- `` batch size`` - 8/16/32\n",
    "\n",
    "Creates new subdirectories under **dlc-models** and appends the project config.yaml file with the correct path to the training and testing pose configuration file. These files hold the parameters for training the network. Such an example file is provided with the toolbox and named as **pose_cfg.yaml**. Things to change in pose_cfg.yaml if you want - this is the default one not the one that is in the train folder\n",
    "- ``global_scale`` - default is 0.8 but change to 1 for low res images\n",
    "- `` batch_size `` - default is 1. but increase batch-size to limit of GPU memory to train for a lower no. of iterations/epochs but not linear.  (can set batch size to 8/16/32)\n",
    "- `` epochs`` - default in pytorch is 200 but can reduce if batch-size is higher\n",
    "- ``pos_dist_thresh`` - default is 17, size of window within which detections are considered positive training samples - non-intuitive\n",
    "- `` pafwidth`` - default is 20, learns associations between pairs of body parts"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eMeUwgxPoEJP",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "deeplabcut.create_training_dataset(path_config_file,augmenter_type='imgaug')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c4FczXGDoEJU"
   },
   "source": [
    "## Start training:\n",
    "\n",
    "This function trains the network for a specific shuffle of the training dataset.  The commented out portion- caps the GPU use if needed. This is only in tensor flow not pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_pOvDq_2oEJW"
   },
   "outputs": [],
   "source": [
    "deeplabcut.train_network(path_config_file,allow_growth=True) #Allow growth =true dynamically grows GPU memory as needed\n",
    "\n",
    "#Can also cap GPU to only use a fraction of the memory Uncomment the section below\n",
    "#import tensorflow as tf\n",
    "#gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.25) # only uses 1/4th of the GPU\n",
    "#sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xZygsb2DoEJc"
   },
   "source": [
    "## Start evaluating\n",
    "This function evaluates a trained model for a specific shuffle/shuffles at a particular state or all the states on the data set (images)\n",
    "and stores the results as .csv file in a subdirectory under **evaluation-results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nv4zlbrnoEJg"
   },
   "outputs": [],
   "source": [
    "deeplabcut.evaluate_network(path_config_file, plotting=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OVFLSKKfoEJk"
   },
   "source": [
    "## Start Analyzing videos\n",
    "This function analyzes the new video. The user can choose the best model from the evaluation results and specify the correct snapshot index for the variable **snapshotindex** in the **config.yaml** file. Otherwise, by default the most recent snapshot is used to analyse the video.\n",
    "The results are stored in hd5 file in the same directory where the video resides.\n",
    "- If split_videos is set to True then the videos are cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y_LZiS_0oEJl"
   },
   "outputs": [],
   "source": [
    "# Videos may need to be broken up if they are too long\n",
    "import deeplabcut\n",
    "import os\n",
    "from deeplabcut.utils.auxfun_videos import VideoWriter\n",
    "\n",
    "split_videos=True\n",
    "if split_videos:\n",
    "    video_path = ['videos/video3.avi','videos/video4.avi'] #Enter a folder OR a list of videos to analyze.\n",
    "    _, ext = os.path.splitext(video_path)\n",
    "    vid = VideoWriter(video_path)\n",
    "    clips = vid.split(n_splits=10)\n",
    "\n",
    "#Run Analysis on the videos\n",
    "deeplabcut.analyze_videos(path_config_file, clips, ext)\n",
    "# Alternate just run videos\n",
    "deeplabcut.analyze_videos(path_config_file,video_path, videotype='.avi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iGu_PdTWoEJr"
   },
   "source": [
    "## Extract outlier frames [optional step]\n",
    "\n",
    "This is an optional step and is used only when the evaluation results are poor i.e. the labels are incorrectly predicted. In such a case, the user can use the following function to extract frames where the labels are incorrectly predicted. This step has many options, so please look at:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplabcut.extract_outlier_frames?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gkbaBOJVoEJs"
   },
   "outputs": [],
   "source": [
    "deeplabcut.extract_outlier_frames(path_config_file,['/videos/video3.avi']) #pass a specific video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8ib0uvhaoEJx"
   },
   "source": [
    "## Refine Labels [optional step]\n",
    "Following the extraction of outlier frames, the user can use the following function to move the predicted labels to the correct location. Thus augmenting the training dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n_FpEXtyoEJy"
   },
   "outputs": [],
   "source": [
    "%gui qt6\n",
    "deeplabcut.refine_labels(path_config_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** Afterwards, if you want to look at the adjusted frames, you can load them in the main GUI by running: ``deeplabcut.label_frames(path_config_file)``\n",
    "\n",
    "\n",
    "#### Once all folders are relabeled, check the labels again! If you are not happy, adjust them in the main GUI:\n",
    "\n",
    "``deeplabcut.label_frames(path_config_file)``\n",
    "\n",
    "Check Labels:\n",
    "\n",
    "``deeplabcut.check_labels(path_config_file)``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CHzstWr8oEJ2",
    "ExecuteTime": {
     "start_time": "2024-11-25T13:45:48.666719Z",
     "end_time": "2024-11-25T13:45:48.749709Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following folder was not manually refined,... C:\\Users\\sahanasrivathsa\\Documents\\SYNC\\Work\\BarnesLab\\CODE\\DEEPLABCUT\\WMaze-SS-2024-11-25\\labeled-data\\VT1\n",
      "Please label, or remove the un-corrected folders.\n"
     ]
    }
   ],
   "source": [
    "#NOW, merge this with your original data:\n",
    "\n",
    "deeplabcut.merge_datasets(path_config_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QCHj7qyboEJ6"
   },
   "source": [
    "## Create a new iteration of training dataset [optional step]\n",
    "Following the refinement of labels and appending them to the original dataset, this creates a new iteration of training dataset. This is automatically set in the config.yaml file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ytQoxIldoEJ7"
   },
   "outputs": [],
   "source": [
    "deeplabcut.create_training_dataset(path_config_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pCrUvQIvoEKD"
   },
   "source": [
    "## Create labeled video\n",
    "This function is for visualiztion purpose and can be used to create a video in .mp4 format with labels predicted by the network. This video is saved in the same directory where the original video resides. \n",
    "\n",
    "THIS HAS MANY FUN OPTIONS! \n",
    "\n",
    "``deeplabcut.create_labeled_video(config, videos, videotype='avi', shuffle=1, trainingsetindex=0, filtered=False, save_frames=False, Frames2plot=None, delete=False, displayedbodyparts='all', codec='mp4v', outputframerate=None, destfolder=None, draw_skeleton=False, trailpoints=0, displaycropped=False)``\n",
    "\n",
    "So please check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplabcut.create_labeled_video?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6aDF7Q7KoEKE"
   },
   "outputs": [],
   "source": [
    "deeplabcut.create_labeled_video(path_config_file,videofile_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8GTiuJESoEKH"
   },
   "source": [
    "## Plot the trajectories of the analyzed videos\n",
    "This function plots the trajectories of all the body parts across the entire video. Each body part is identified by a unique color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gX21zZbXoEKJ"
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook #for making interactive plots.\n",
    "deeplabcut.plot_trajectories(path_config_file,videofile_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## BATCH PROCESSING\n",
    "\n",
    "This script can run a video analysis over all folders"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import deeplabcut\n",
    "\n",
    "def getsubfolders(folder):\n",
    "    ''' returns list of subfolders '''\n",
    "    return [os.path.join(folder,p) for p in os.listdir(folder) if os.path.isdir(os.path.join(folder,p))]\n",
    "\n",
    "project='ComplexWheelD3-12-Fumi-2019-01-28'\n",
    "\n",
    "shuffle=1\n",
    "\n",
    "prefix='/home/alex/DLC-workshopRowland'\n",
    "\n",
    "projectpath=os.path.join(prefix,project)\n",
    "config=os.path.join(projectpath,'config.yaml')\n",
    "\n",
    "basepath='/home/alex/BenchmarkingExperimentsJan2019' #data'\n",
    "\n",
    "'''\n",
    "\n",
    "Imagine that the data (here: videos of 3 different types) are in subfolders:\n",
    "    /January/January29 ..\n",
    "    /February/February1\n",
    "    /February/February2\n",
    "\n",
    "    etc.\n",
    "\n",
    "'''\n",
    "\n",
    "subfolders=getsubfolders(basepath)\n",
    "for subfolder in subfolders: #this would be January, February etc. in the upper example\n",
    "    print(\"Starting analyze data in:\", subfolder)\n",
    "    subsubfolders=getsubfolders(subfolder)\n",
    "    for subsubfolder in subsubfolders: #this would be Febuary1, etc. in the upper example...\n",
    "        print(\"Starting analyze data in:\", subsubfolder)\n",
    "        for vtype in ['.mp4','.m4v','.mpg']:\n",
    "            deeplabcut.analyze_videos(config,[subsubfolder],shuffle=shuffle,videotype=vtype,save_as_csv=True)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Demo-yourowndata.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
